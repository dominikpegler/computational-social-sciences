{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGJ-Ol0sqtgY"
   },
   "source": [
    "## 1. Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JCgDbzKkUgWO"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "import inflect\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Download if necessary\n",
    "try:\n",
    "    os.path.exists(os.path.expanduser(\"~/nltk_data/\"))\n",
    "except:\n",
    "    nltk.download(\"all\")\n",
    "\n",
    "\n",
    "string = \"Hello e-v-e-r-y-o-n-e!!!@@@!!!!! @DON'T BUY THIS PHONE at all-first of all that say the phone is new I took it to the lab after 6 month the phone is dead dead , you can save it open the phone in the lab and say!!!!the phone is renew , and its cheepe components. I paid 400$ for only 6 month , now I need to buy new one this LG G4 is dead . Not nice people say to me don't buy from https://www.amazon.com/asdasdas/asdas at all!!! it's troubling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NA-xq4UIyyIf"
   },
   "source": [
    "General note: In the next cells we define a function and then apply the same function.\n",
    "\n",
    "Defining the function has the syntax#\n",
    "\n",
    "def name_of_the_funtion(input):\n",
    "  operation\n",
    "  return (output of the operation)\n",
    "\n",
    "In our case operations are substitutions of string elements by empty spaces ''\n",
    "\n",
    "Once the function is defined, we can call it and apply it to a selected input and get an output.\n",
    "\n",
    "After the function is defined once, we don't need to define it again, we can just call it (as we do below)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GI7HPEXoUW8E",
    "outputId": "d12fb849-2814-48f7-de1d-efe0f582908a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello e-v-e-r-y-o-n-e!!!@@@!!!!! @DON'T BUY THIS PHONE at all-first of all that say the phone is new I took it to the lab after 6 month the phone is dead dead , you can save it open the phone in the lab and say!!!!the phone is renew , and its cheepe components. I paid 400$ for only 6 month , now I need to buy new one this LG G4 is dead . Not nice people say to me don't buy from  at all!!! it's troubling\n"
     ]
    }
   ],
   "source": [
    "## This function removes any mention to website\n",
    "## this application of re.sub() substitutes strings in the input starting with http with ''\n",
    "\n",
    "\n",
    "def clean_url(input):\n",
    "    output = re.sub(r\"http\\S+\", \"\", input)\n",
    "    return output\n",
    "\n",
    "\n",
    "## we take the original string and then apply the function clean_url, obtaining the string after_url_clean\n",
    "after_url_clean = clean_url(string)\n",
    "\n",
    "## We print the string after removing url\n",
    "print(after_url_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XjepjZlBGYx",
    "outputId": "5e048e87-2128-4bb5-8486-d98ab7ae771d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello e-v-e-r-y-o-n-e!!!@@@!!!!! @DO NOT BUY THIS PHONE at all-first of all that say the phone is new I took it to the lab after 6 month the phone is dead dead , you can save it open the phone in the lab and say!!!!the phone is renew , and its cheepe components. I paid 400$ for only 6 month , now I need to buy new one this LG G4 is dead . Not nice people say to me do not buy from  at all!!! it is troubling\n"
     ]
    }
   ],
   "source": [
    "def fix_contraction(input):\n",
    "    output = contractions.fix(input)\n",
    "    return output\n",
    "\n",
    "\n",
    "## we store the fixed string into the variable contractions_fixed and print it\n",
    "\n",
    "contractions_fixed = fix_contraction(after_url_clean)\n",
    "print(contractions_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THR9wI_MWJhZ",
    "outputId": "39650699-65f3-4752-9c25-7fcccad33b45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello e v e r y o n e             DO NOT BUY THIS PHONE at all first of all that say the phone is new I took it to the lab after 6 month the phone is dead dead   you can save it open the phone in the lab and say    the phone is renew   and its cheepe components  I paid 400  for only 6 month   now I need to buy new one this LG G4 is dead   Not nice people say to me do not buy from  at all    it is troubling\n"
     ]
    }
   ],
   "source": [
    "def clean_non_alphanumeric(input):\n",
    "    output = re.sub(r\"[^a-zA-Z0-9]\", \" \", input)\n",
    "    return output\n",
    "\n",
    "\n",
    "only_alphanumeric = clean_non_alphanumeric(contractions_fixed)\n",
    "\n",
    "print(only_alphanumeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ieTJw3AlWbyi",
    "outputId": "8756421c-19e4-49bd-8cda-c1d1ca8491fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello e v e r y o n e             do not buy this phone at all first of all that say the phone is new i took it to the lab after 6 month the phone is dead dead   you can save it open the phone in the lab and say    the phone is renew   and its cheepe components  i paid 400  for only 6 month   now i need to buy new one this lg g4 is dead   not nice people say to me do not buy from  at all    it is troubling\n"
     ]
    }
   ],
   "source": [
    "def clean_lowercase(input):\n",
    "    output = str(input).lower()\n",
    "    return output\n",
    "\n",
    "\n",
    "lower_cased = clean_lowercase(only_alphanumeric)\n",
    "print(lower_cased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TGK4v4VWjqp",
    "outputId": "a836c6e6-375f-4d91-df45-a51913fe74fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'e', 'v', 'e', 'r', 'y', 'o', 'n', 'e', 'do', 'not', 'buy', 'this', 'phone', 'at', 'all', 'first', 'of', 'all', 'that', 'say', 'the', 'phone', 'is', 'new', 'i', 'took', 'it', 'to', 'the', 'lab', 'after', '6', 'month', 'the', 'phone', 'is', 'dead', 'dead', 'you', 'can', 'save', 'it', 'open', 'the', 'phone', 'in', 'the', 'lab', 'and', 'say', 'the', 'phone', 'is', 'renew', 'and', 'its', 'cheepe', 'components', 'i', 'paid', '400', 'for', 'only', '6', 'month', 'now', 'i', 'need', 'to', 'buy', 'new', 'one', 'this', 'lg', 'g4', 'is', 'dead', 'not', 'nice', 'people', 'say', 'to', 'me', 'do', 'not', 'buy', 'from', 'at', 'all', 'it', 'is', 'troubling']\n"
     ]
    }
   ],
   "source": [
    "def clean_tokenization(input):\n",
    "    output = nltk.word_tokenize(input)\n",
    "    return output\n",
    "\n",
    "\n",
    "tokenized = clean_tokenization(lower_cased)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wJ294xAW3wI",
    "outputId": "bf5b81fe-3926-49d4-beba-8738ee190413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "## We can have a look at the list of english stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMBKc3_oW9pn",
    "outputId": "aa60d575-b27a-45e2-a207-8988f0d22c4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'e', 'v', 'e', 'r', 'n', 'e', 'buy', 'phone', 'first', 'say', 'phone', 'new', 'took', 'lab', '6', 'month', 'phone', 'dead', 'dead', 'save', 'open', 'phone', 'lab', 'say', 'phone', 'renew', 'cheepe', 'components', 'paid', '400', '6', 'month', 'need', 'buy', 'new', 'one', 'lg', 'g4', 'dead', 'nice', 'people', 'say', 'buy', 'troubling']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def clean_stopwords(input):\n",
    "    output = [item for item in input if item not in stop_words]\n",
    "    return output\n",
    "\n",
    "\n",
    "without_stop_words = clean_stopwords(tokenized)\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tm9mUrrSFpBP",
    "outputId": "17556a7e-607f-4445-8b51-66452f330bd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'e', 'v', 'e', 'r', 'n', 'e', 'buy', 'phone', 'first', 'say', 'phone', 'new', 'took', 'lab', 'six', 'month', 'phone', 'dead', 'dead', 'save', 'open', 'phone', 'lab', 'say', 'phone', 'renew', 'cheepe', 'components', 'paid', 'four hundred', 'six', 'month', 'need', 'buy', 'new', 'one', 'lg', 'g4', 'dead', 'nice', 'people', 'say', 'buy', 'troubling']\n"
     ]
    }
   ],
   "source": [
    "p = inflect.engine()\n",
    "\n",
    "\n",
    "def numbers_to_words(input):\n",
    "    output = []\n",
    "    for item in input:\n",
    "        if item.isnumeric() == True:\n",
    "            output += [p.number_to_words(item)]\n",
    "        else:\n",
    "            output += [item]\n",
    "    return output\n",
    "\n",
    "\n",
    "fixed_numbers = numbers_to_words(without_stop_words)\n",
    "print(fixed_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_77kue6XnvP",
    "outputId": "28b5830c-aa78-46d3-c7bc-c4c762ed81d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'e', 'v', 'e', 'r', 'n', 'e', 'buy', 'phone', 'first', 'say', 'phone', 'new', 'take', 'lab', 'six', 'month', 'phone', 'dead', 'dead', 'save', 'open', 'phone', 'lab', 'say', 'phone', 'renew', 'cheepe', 'components', 'pay', 'four hundred', 'six', 'month', 'need', 'buy', 'new', 'one', 'lg', 'g4', 'dead', 'nice', 'people', 'say', 'buy', 'trouble']\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_lemmatization(input):\n",
    "    output = [lemma.lemmatize(word=w, pos=\"v\") for w in input]\n",
    "    return output\n",
    "\n",
    "\n",
    "lemmatized = clean_lemmatization(fixed_numbers)\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxZymLacYCEm",
    "outputId": "e8a75764-8aa4-47d1-a92e-9167353aea9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'buy', 'phone', 'first', 'say', 'phone', 'new', 'take', 'lab', 'six', 'month', 'phone', 'dead', 'dead', 'save', 'open', 'phone', 'lab', 'say', 'phone', 'renew', 'cheepe', 'components', 'pay', 'four hundred', 'six', 'month', 'need', 'buy', 'new', 'one', 'dead', 'nice', 'people', 'say', 'buy', 'trouble']\n"
     ]
    }
   ],
   "source": [
    "def clean_length(input):\n",
    "    output = [word for word in input if len(word) > 2]\n",
    "    return output\n",
    "\n",
    "\n",
    "bigger_than_two = clean_length(lemmatized)\n",
    "print(bigger_than_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yKJzyIjzR7Yo",
    "outputId": "7cb80dbd-b181-4a14-97bb-08a6f03b89ff"
   },
   "source": [
    "## COUNT WORD FREQUENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yKJzyIjzR7Yo",
    "outputId": "7cb80dbd-b181-4a14-97bb-08a6f03b89ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('phone', 0.13513513513513514), ('buy', 0.08108108108108109), ('say', 0.08108108108108109), ('dead', 0.08108108108108109), ('new', 0.05405405405405406), ('lab', 0.05405405405405406), ('six', 0.05405405405405406), ('month', 0.05405405405405406), ('hello', 0.02702702702702703), ('first', 0.02702702702702703), ('take', 0.02702702702702703), ('save', 0.02702702702702703), ('open', 0.02702702702702703), ('renew', 0.02702702702702703), ('cheepe', 0.02702702702702703), ('components', 0.02702702702702703), ('pay', 0.02702702702702703), ('four hundred', 0.02702702702702703), ('need', 0.02702702702702703), ('one', 0.02702702702702703), ('nice', 0.02702702702702703), ('people', 0.02702702702702703), ('trouble', 0.02702702702702703)]\n"
     ]
    }
   ],
   "source": [
    "def count_word_frequencies(input):\n",
    "    # we get the word count\n",
    "    word_count = nltk.FreqDist(input)\n",
    "    # we build a list of word counts, from more frequent word to less frequent\n",
    "    word_count_most_common = word_count.most_common()\n",
    "    # to get the frequencies, we need to divide the word count by the number of words in the sentence, i.e. len(input)\n",
    "    output = [(count[0], count[1] / len(input)) for count in word_count_most_common]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "word_frequencies = count_word_frequencies(bigger_than_two)\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "FjC7LC6MYMpE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello buy phone first say phone new take lab six month phone dead dead save open phone lab say phone renew cheepe components pay four hundred six month need buy new one dead nice people say buy trouble\n"
     ]
    }
   ],
   "source": [
    "def convert_to_string(input):\n",
    "    output = \" \".join(input)\n",
    "    return output\n",
    "\n",
    "\n",
    "filtered_string = convert_to_string(bigger_than_two)\n",
    "print(filtered_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPXcnAiAX4Cx"
   },
   "source": [
    "## Now we can use all these functions within a higher level function, so that we can apply the whole preprocessing pipeline iteratively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77qMf4v-fR2w",
    "outputId": "33e9fb3c-5ef5-477b-e01c-c01f83dcee91"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemma = WordNetLemmatizer()\n",
    "p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pTd3fFFeZfAj"
   },
   "outputs": [],
   "source": [
    "## WE DEFINE ALL THE FUNCTIONS WE WILL NEED\n",
    "\n",
    "## This function removes any mention to website\n",
    "## this application of re.sub() substitutes strings in the input starting with http with ''\n",
    "def clean_url(input):\n",
    "    output = re.sub(r\"http\\S+\", \"\", input)\n",
    "    return output\n",
    "\n",
    "\n",
    "## This function fixes contractions - turns \"don't\" into \"do not\", or \"can't\" into \"can not\"\n",
    "def fix_contraction(input):\n",
    "    output = contractions.fix(input)\n",
    "    return output\n",
    "\n",
    "\n",
    "## This function finds all non-alphanumeric characters [^a-zA-Z] and substitutes them with ''\n",
    "def clean_non_alphanumeric(input):\n",
    "    output = re.sub(r\"[^a-zA-Z0-9]\", \" \", input)\n",
    "    return output\n",
    "\n",
    "\n",
    "## This function takes a string with both lower and upper case elements and returns only lower case elements\n",
    "def clean_lowercase(input):\n",
    "    output = str(input).lower()\n",
    "    return output\n",
    "\n",
    "\n",
    "## The function tokenize takes a string as input and returns a list of tokens\n",
    "def clean_tokenization(input):\n",
    "    output = nltk.word_tokenize(input)\n",
    "    return output\n",
    "\n",
    "\n",
    "## eliminate stop words\n",
    "def clean_stopwords(input):\n",
    "    output = [item for item in input if item not in stop_words]\n",
    "    return output\n",
    "\n",
    "\n",
    "## This function turns numeric values into words, \"6\" into \"six\"\n",
    "def numbers_to_words(input):\n",
    "    output = []\n",
    "    for item in input:\n",
    "        if item.isnumeric() == True:\n",
    "            output += [p.number_to_words(item)]\n",
    "        else:\n",
    "            output += [item]\n",
    "    return output\n",
    "\n",
    "\n",
    "## Lemmatize tokens\n",
    "def clean_lemmatization(input):\n",
    "    output = [lemma.lemmatize(word=w, pos=\"v\") for w in input]\n",
    "    return output\n",
    "\n",
    "\n",
    "## in this function we create a new list that includes only words with lenght > 2\n",
    "def clean_length(input):\n",
    "    output = [word for word in input if len(word) > 2]\n",
    "    return output\n",
    "\n",
    "\n",
    "## COUNT WORD FREQUENCIES\n",
    "def count_word_frequencies(input):\n",
    "    word_count = nltk.FreqDist(input)\n",
    "    word_count_most_common = word_count.most_common()\n",
    "    output = [(count[0], count[1] / len(input)) for count in word_count_most_common]\n",
    "    return output\n",
    "\n",
    "\n",
    "## finally, after al the filtering steps, we can put the string back together\n",
    "def convert_to_string(input):\n",
    "    output = \" \".join(input)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Lm_DUf2ra80S"
   },
   "outputs": [],
   "source": [
    "## We define the pipeline as a function\n",
    "\n",
    "\n",
    "def pre_processing_pipeline(input):\n",
    "    w = clean_url(input)\n",
    "    w = fix_contraction(w)\n",
    "    w = clean_non_alphanumeric(w)\n",
    "    w = clean_lowercase(w)\n",
    "    w = clean_tokenization(w)\n",
    "    w = numbers_to_words(w)\n",
    "    w = clean_stopwords(w)\n",
    "    w = clean_lemmatization(w)\n",
    "    clean_list = clean_length(w)\n",
    "    word_frequencies = count_word_frequencies(clean_list)\n",
    "    filtered_string = convert_to_string(clean_list)\n",
    "\n",
    "    return (filtered_string, word_frequencies, clean_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3i8uqi6d7SL",
    "outputId": "db997be0-b1b1-4bad-abda-2b31bb34f335"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello buy phone first say phone new take lab six month phone dead dead save open phone lab say phone renew cheepe components pay four hundred six month need buy new one dead nice people say buy trouble\n",
      "[('phone', 0.13513513513513514), ('buy', 0.08108108108108109), ('say', 0.08108108108108109), ('dead', 0.08108108108108109), ('new', 0.05405405405405406), ('lab', 0.05405405405405406), ('six', 0.05405405405405406), ('month', 0.05405405405405406), ('hello', 0.02702702702702703), ('first', 0.02702702702702703), ('take', 0.02702702702702703), ('save', 0.02702702702702703), ('open', 0.02702702702702703), ('renew', 0.02702702702702703), ('cheepe', 0.02702702702702703), ('components', 0.02702702702702703), ('pay', 0.02702702702702703), ('four hundred', 0.02702702702702703), ('need', 0.02702702702702703), ('one', 0.02702702702702703), ('nice', 0.02702702702702703), ('people', 0.02702702702702703), ('trouble', 0.02702702702702703)]\n",
      "['hello', 'buy', 'phone', 'first', 'say', 'phone', 'new', 'take', 'lab', 'six', 'month', 'phone', 'dead', 'dead', 'save', 'open', 'phone', 'lab', 'say', 'phone', 'renew', 'cheepe', 'components', 'pay', 'four hundred', 'six', 'month', 'need', 'buy', 'new', 'one', 'dead', 'nice', 'people', 'say', 'buy', 'trouble']\n"
     ]
    }
   ],
   "source": [
    "## we can apply this piece of code, and with one line have the whole pipeline\n",
    "\n",
    "string = \"Hello e-v-e-r-y-o-n-e!!!@@@!!!!! @DON'T BUY THIS PHONE at all-first of all that say the phone is new I took it to the lab after 6 month the phone is dead dead , you can save it open the phone in the lab and say!!!!the phone is renew , and its cheepe components. I paid 400$ for only 6 month , now I need to buy new one this LG G4 is dead . Not nice people say to me don't buy from https://www.amazon.com/asdasdas/asdas at all!!! it's troubling\"\n",
    "\n",
    "preprocessed = pre_processing_pipeline(string)\n",
    "\n",
    "# we can obtain the preprocessed sentence\n",
    "print(preprocessed[0])\n",
    "\n",
    "## OR\n",
    "\n",
    "# we can obtain the word frequencies\n",
    "print(preprocessed[1])\n",
    "\n",
    "## OR\n",
    "\n",
    "# we can obtain the word list\n",
    "print(preprocessed[2])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
