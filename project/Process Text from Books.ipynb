{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583c2c4c-0d47-4d74-a3bf-b3dcd2aac2d2",
   "metadata": {},
   "source": [
    "Process Text from entire books\n",
    "===\n",
    "\n",
    "* This could be the first step of a possible final project  \n",
    "* In a further step text could be analyzed   \n",
    "* Books could be rated according to certain dimensions  \n",
    "* Find the music with the most similar lyrics to a book  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "100daff4-f6e0-4b0f-a334-eee60ba1ac70",
   "metadata": {},
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c89a71e-6a24-47e6-882c-d914e55b41c5",
   "metadata": {},
   "source": [
    "## From PDF to Text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f2cc29d-7460-4f9d-8579-3d1dcd860369",
   "metadata": {},
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdffileobj=open('data/1.pdf','rb')\n",
    " \n",
    "pdfreader=PyPDF2.PdfFileReader(pdffileobj)\n",
    " \n",
    "x=pdfreader.numPages\n",
    " \n",
    "pageobj=pdfreader.getPage(x+1)\n",
    " \n",
    "text=pageobj.extractText()\n",
    " \n",
    "file1=open(r\"data/1.txt\",\"a\")\n",
    "file1.writelines(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db54bd09-2446-43c7-8231-6219b719ea8d",
   "metadata": {},
   "source": [
    "## From Ebook to Text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16a1ab6e-47b5-4c9e-8296-2a223506d6f8",
   "metadata": {},
   "source": [
    "!pip install epub2txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3bf08d35-9689-4416-b137-3ba8d3693191",
   "metadata": {},
   "source": [
    "from epub2txt import epub2txt\n",
    "# from a url to epub\n",
    "url = \"https://github.com/ffreemt/tmx2epub/raw/master/tests/1.tmx.epub\"\n",
    "res = epub2txt(url)\n",
    "\n",
    "# from a local epub file\n",
    "filepath = r\"tests\\test.epub\"\n",
    "res = epub2txt(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8519ee7-26c4-4c55-929c-8dc391fd2ac7",
   "metadata": {},
   "source": [
    "## Books that were already converted from to txt with calibre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b93aad73-564a-4ca4-a211-bd547f31b7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/Thomas Pynchon - V.txt',\n",
       " \"data/Thomas Pynchon - Gravity's rainbow.txt\",\n",
       " 'data/William Faulkner - The Sound and the Fury.txt']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "books = glob(\"data/*.txt\")\n",
    "\n",
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fab7d4fc-b2cb-4ae6-99e9-5a586f604cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book has 6664 paragraphs.\n"
     ]
    }
   ],
   "source": [
    "with open('data/William Faulkner - The Sound and the Fury.txt', \"r\") as fo:\n",
    "    book = fo.readlines()\n",
    "\n",
    "print(f\"Book has {len(book)} paragraphs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1fe75b-d43c-47fb-a562-d1d7180038cf",
   "metadata": {},
   "source": [
    "### Inspection\n",
    "1. Novel starts at paragraph 58\n",
    "2. There are meaningless numbers in the format \"1.8\" in between the text on single lines.\n",
    "3. Each paragraph ends with `\\n` which is not necessary here\n",
    "4. There are entirely empty lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34e80fa-3f34-4397-8433-45b91c2c7244",
   "metadata": {},
   "source": [
    "#### 1. Trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8ddcdbec-cdaf-49ad-848f-8ab14b1ba290",
   "metadata": {},
   "outputs": [],
   "source": [
    "book = book[58:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb8721-780c-46c9-8f78-0994638c896f",
   "metadata": {},
   "source": [
    "#### 2. Remove meaningless numbers and 3. remove `\\n`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb536d-e085-453a-941c-ce567552b134",
   "metadata": {},
   "source": [
    "*Example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "65740fa5-c1cf-482b-b47f-c5fa010b92f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.1\\n'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "410be7c2-b8a2-4e68-b9a4-b28a5d7644c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'April 7, 1928\\n'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "80f9ab8b-7025-452a-bdbf-c9cf87bcfa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "rm_pattern_1 = \"^[0-9]*\\.[0-9]*\\\\n$\"\n",
    "rm_pattern_2 = \"\\\\n$\"\n",
    "\n",
    "for i, _ in enumerate(book):\n",
    "    book[i] = re.sub(rm_pattern_1, \"\", book[i]) \n",
    "    book[i] = re.sub(rm_pattern_2, \"\", book[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fb6b84-4cba-4924-af3c-ec26a68b4ae6",
   "metadata": {},
   "source": [
    "*Example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "78a3edb6-c2a5-41aa-a6b2-f604443723be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bd1c3225-ce4c-4a13-950b-72088d6d89e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'April 7, 1928'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a2c26-6c79-4e4d-8621-e5df7b91e9a7",
   "metadata": {},
   "source": [
    "#### 4. Remove empty lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c0f58-65f1-4312-9d8f-67cfacb34bd9",
   "metadata": {},
   "source": [
    "*Before*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c2cb741b-37c1-41de-9db6-fd74055f62b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 7, 1928\n",
      "\n",
      "\n",
      "\n",
      "Through the fence, between the curling flower spaces, I could see them hitting. They were coming toward where the flag was and I went along the fence. Luster was hunting in the grass by the flower tree. They took the flag out, and they were hitting. Then they put the flag back and they went to the table, and he hit and the other hit. Then they went on, and I went along the fence. Luster came away from the flower tree and we went along the fence and they stopped and we stopped and I looked through the fence while Luster was hunting in the grass.\n",
      "\n",
      "\"Here, caddie.\" He hit. They went away across the pasture. I held to the fence and watched them going away.\n"
     ]
    }
   ],
   "source": [
    "for p in book[:7]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c32b2-c233-4623-94d6-c499f6b91659",
   "metadata": {},
   "source": [
    "*After*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6b922f1d-758c-4f6f-99d7-891b7ecbf83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 7, 1928\n",
      "Through the fence, between the curling flower spaces, I could see them hitting. They were coming toward where the flag was and I went along the fence. Luster was hunting in the grass by the flower tree. They took the flag out, and they were hitting. Then they put the flag back and they went to the table, and he hit and the other hit. Then they went on, and I went along the fence. Luster came away from the flower tree and we went along the fence and they stopped and we stopped and I looked through the fence while Luster was hunting in the grass.\n",
      "\"Here, caddie.\" He hit. They went away across the pasture. I held to the fence and watched them going away.\n"
     ]
    }
   ],
   "source": [
    "book_trimmed = [p for p in book if p != '']\n",
    "\n",
    "for p in book_trimmed[:3]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a630a4-e227-48f3-aa6d-8904ee49b716",
   "metadata": {},
   "source": [
    "## Start analyzing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d7d40f-cc19-4ea3-afd8-9e3ea89eda5b",
   "metadata": {},
   "source": [
    "### Fix contractions & lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b2f9bd2f-43dd-407a-8702-7d40cb4887f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "\n",
    "for i, _ in enumerate(book_trimmed):\n",
    "    book_trimmed[i] = contractions.fix(book_trimmed[i]).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2278405-6541-460e-bbaa-c9d19e45ffb5",
   "metadata": {},
   "source": [
    "### Tokenize paragraphs to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "35370eb9-8fae-4a16-b3f2-7ed95809ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from copy import deepcopy\n",
    "\n",
    "book_trimmed_tokenized = deepcopy(book_trimmed)\n",
    "\n",
    "for i, p in enumerate(book_trimmed):\n",
    "    book_trimmed_tokenized[i] = nltk.sent_tokenize(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab11e51a-cb48-4ac3-bef9-29ce8440ddec",
   "metadata": {},
   "source": [
    "### Sentiment scores for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "67cecca4-5eae-461c-a4d9-5e721935e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8903d66-b3d2-45c1-8ba6-39fc2abc56d3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">Let's see what is the difference between computing the score over the individual sentences or for the whole paragraph. It seems that this is an absolute value that accumulates the longer a text. High scores for long texts can thus be misleading. Is there a relative measure? Dividing by number of words or sentences? That would be necessary to make valid comparisons. I.e. 2 books could have both positive scores, but one has higher scores. Could it be because of its length?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ec5fa3db-adb1-486b-af21-246c66408f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A) whole paragraph:\n",
      "\n",
      "it was red, flapping on the pasture. then there was a bird slanting and tilting on it. luster threw. the flag flapped on the bright grass and the trees. i held to the fence.\n",
      "=> 0.4404\n",
      "\n",
      "B) single sentences:\n",
      "\n",
      "it was red, flapping on the pasture.\n",
      "=> 0.0\n",
      "then there was a bird slanting and tilting on it.\n",
      "=> 0.0\n",
      "luster threw.\n",
      "=> 0.0\n",
      "the flag flapped on the bright grass and the trees.\n",
      "=> 0.4404\n",
      "i held to the fence.\n",
      "=> 0.0\n"
     ]
    }
   ],
   "source": [
    "idx = 6\n",
    "\n",
    "print(\"A) whole paragraph:\\n\")\n",
    "print(book_trimmed[idx])\n",
    "print(\"=>\", sid.polarity_scores(book_trimmed[idx])['compound'])\n",
    "\n",
    "print(\"\\nB) single sentences:\\n\")\n",
    "for sent in book_trimmed_tokenized[idx]:\n",
    "    print(sent)\n",
    "    print(\"=>\", sid.polarity_scores(sent)['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c253eff-8898-4000-a701-f916a9bc13eb",
   "metadata": {},
   "source": [
    "### Numbers to words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed2549-bee2-490a-b10a-aa4e1a55cf47",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">Do we need the this?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf1aaf-8fb8-4e25-8824-f30253f7c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "inf_engine =  inflect.engine()\n",
    "for i, _ in enumerate(book_trimmed):\n",
    "    book_trimmed[i] = inf_engine.number_to_words(book_trimmed[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
