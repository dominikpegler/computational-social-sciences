{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6e9f5a-f7e1-4d46-bdaa-c1b51585ce8e",
   "metadata": {},
   "source": [
    "Social media scraping\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd21ee82-8fd4-4c0f-bf38-37cab1108235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "data_dir = \"data/twitter\"\n",
    "\n",
    "if os.path.exists(data_dir) == False:\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0c0f1-67cd-4491-8901-0330b8dbddf7",
   "metadata": {},
   "source": [
    "## Tweets by User"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d913f65b-3daf-480a-94ad-035d4eb8b5ca",
   "metadata": {},
   "source": [
    "Scrape the last 100 tweets from `@derstandard` and store in json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a19df21c-a13d-4e28-b349-422754a9b31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 100 tweets\n",
      "\n",
      "2023-01-22T19:49:33+00:00 XSudoku mittel 5422b:  https://t.co/JpypjJAZiU None\n",
      "2023-01-22T19:17:17+00:00 Uhrmacher: \"Die Kundschaft glaubt, mir die Arbeit erklären zu müssen\":  https://t.co/XgBdv9G5Qs None\n",
      "2023-01-22T18:02:02+00:00 Weiterarbeiten im Pensionsalter: \"Meine Arbeit ist mein Leben\":  https://t.co/C1zpXtuZ9j None\n",
      "2023-01-22T18:02:01+00:00 Ältere sollen die Joblücke füllen. Aber wer will sie wirklich haben?:  https://t.co/a0yvyd7NAQ None\n",
      "2023-01-22T17:31:23+00:00 Julian Schütter: Kitzbühel-Opfer und Skifahrer for Future:  https://t.co/YEY11CcFhs None\n"
     ]
    }
   ],
   "source": [
    "!snscrape --jsonl --max-results 100 twitter-user derstandardat > data/twitter/user-derstandardat.json\n",
    "\n",
    "\n",
    "with open(data_dir+\"/user-derstandardat.json\", \"r\") as fo:\n",
    "    derstandard_tweets = fo.readlines()\n",
    "\n",
    "derstandard_tweets = [json.loads(derstandard_tweets[i]) for i in range(0, len(derstandard_tweets))]\n",
    "\n",
    "\n",
    "print(\"loaded\", len(derstandard_tweets), \"tweets\\n\")\n",
    "\n",
    "\n",
    "# print the first 5\n",
    "for i in range(min(5, len(derstandard_tweets))):\n",
    "    print(derstandard_tweets[i][\"date\"], derstandard_tweets[i][\"rawContent\"], derstandard_tweets[i][\"hashtags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c83eac-718a-49b8-9e0e-d42f253c009d",
   "metadata": {},
   "source": [
    "## Tweets by Hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f37a6d-2dc0-4003-8c3c-0beced29fa5c",
   "metadata": {},
   "source": [
    "Scrape the last 100 tweets with `#chinesevirus` and store in json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10c7c878-d5f3-4af3-bed6-81ae2ebb541c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 100 tweets\n",
      "\n",
      "2023-01-22T20:41:50+00:00 @EpochTimesEs #chinesevirus ['chinesevirus']\n",
      "2023-01-22T18:41:36+00:00 #chineseVirus ['chineseVirus']\n",
      "2023-01-22T15:30:19+00:00 #ChineseVirus update https://t.co/fkpN3sRSrL ['ChineseVirus']\n",
      "2023-01-22T15:02:11+00:00 @AmbassadeChine Year of Chinese BAT\n",
      "#chinesevirus ['chinesevirus']\n",
      "2023-01-22T13:10:58+00:00 #ChineseNewYear #ChineseVirus ['ChineseNewYear', 'ChineseVirus']\n"
     ]
    }
   ],
   "source": [
    "!snscrape --jsonl --max-results 100 twitter-hashtag chinesevirus > data/twitter/hashtag-chinesevirus.json\n",
    "\n",
    "\n",
    "with open(data_dir+\"/hashtag-chinesevirus.json\", \"r\") as fo:\n",
    "    tweets = fo.readlines()\n",
    "\n",
    "tweets = [json.loads(tweets[i]) for i in range(0, len(tweets))]\n",
    "\n",
    "\n",
    "print(\"loaded\", len(tweets), \"tweets\\n\")\n",
    "\n",
    "\n",
    "# print the first 5\n",
    "for i in range(min(5, len(tweets))):\n",
    "    print(tweets[i][\"date\"], tweets[i][\"rawContent\"], tweets[i][\"hashtags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb14dee9-b0cf-41a3-97f0-ded14077aa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 3 tweets\n",
      "\n",
      "2020-10-20T21:34:56+00:00 #Plandemie ist kein #SchlitzaugenVirus @StefanPetzner im Interview https://t.co/7zb8NN9NF4 ['Plandemie', 'SchlitzaugenVirus']\n",
      "2020-08-17T07:39:46+00:00 @stefan_petzner @cbischofberger Mit rassistischen Sagern würde man „die Menschen“ wohl besser erreichen, was? #schlitzaugenvirus ['schlitzaugenvirus']\n",
      "2020-06-03T16:35:49+00:00 Man muss schon ein extremer Realitätsverweigerer sein, wenn man sein Profil, trotz des längst gescheiterten schwedischen Weges, mit einer Schwedenflagge schmückt.\n",
      "\n",
      "Oder hat es gar mit schwedischen Gardinen zu tun?\n",
      "#SchlitzaugenVirus https://t.co/uZpbYtOnpN ['SchlitzaugenVirus']\n"
     ]
    }
   ],
   "source": [
    "!snscrape --jsonl --max-results 100 twitter-hashtag schlitzaugenvirus > data/twitter/hashtag-schlitzaugenvirus.json\n",
    "\n",
    "\n",
    "with open(data_dir+\"/hashtag-schlitzaugenvirus.json\", \"r\") as fo:\n",
    "    tweets = fo.readlines()\n",
    "\n",
    "tweets = [json.loads(tweets[i]) for i in range(0, len(tweets))]\n",
    "\n",
    "\n",
    "print(\"loaded\", len(tweets), \"tweets\\n\")\n",
    "\n",
    "\n",
    "# print the first 5\n",
    "for i in range(min(5, len(tweets))):\n",
    "    print(tweets[i][\"date\"], tweets[i][\"rawContent\"], tweets[i][\"hashtags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27f79835-af01-4cd1-a022-4e90b1bfc968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 100 tweets\n",
      "\n",
      "2023-01-22T20:52:27+00:00 Gestern in der Sauna. \n",
      "Bin ja  als #Ungeimpfter jeden weiteren Tag entspannter und gelassener. \n",
      "Wenn aber ein Honk behauptet, dass die #Plandemie nur deswegen so massiv war, weil die #Ungeimpften die #Geimpften infiziert haben, kann ich nicht anders als zu reagieren. ['Ungeimpfter', 'Plandemie', 'Ungeimpften', 'Geimpften']\n",
      "2023-01-22T20:39:46+00:00 #LucMontagnier #VIH #Covid\n",
      "#Plandemic #plandemia #plandemie #plandemi https://t.co/HwILgcTnNS ['LucMontagnier', 'VIH', 'Covid', 'Plandemic', 'plandemia', 'plandemie', 'plandemi']\n",
      "2023-01-22T20:39:06+00:00 Diesem Mann sollte man zuhören. Er hat die Farce der #Schweinegrippe bereits durchschaut. Mit ihm als #Gesundheitsminister hätte es die #PLANdemie niemals gegeben. Lest auch sein Buch: Falsche Pandemien. #Wodarg #LongCovid #Impfschäden ['Schweinegrippe', 'Gesundheitsminister', 'PLANdemie', 'Wodarg', 'LongCovid', 'Impfschäden']\n",
      "2023-01-22T18:12:12+00:00 Ein #Vorteil der #Corona-#Plandemie ist es, dass viele am #Anfang der #Woche garantiert zu ihrem #Spaziergang an der frischen #Luft kommen.\n",
      "#Gesundheit #NatürlichesImmunsystem ['Vorteil', 'Corona', 'Plandemie', 'Anfang', 'Woche', 'Spaziergang', 'Luft', 'Gesundheit', 'NatürlichesImmunsystem']\n",
      "2023-01-22T17:32:31+00:00 #Afrika ist der große Gewinner der #Corona-#Plandemie. Deswegen versuchen sie beim nächsten letzten #Anlauf, auch diesen #Kontinent in den #Griff zu bekommen. Sie werden #scheitern.\n",
      "\"#Impfung\" #Pandemievertrag #WHO #WEF #BigPharma ['Afrika', 'Corona', 'Plandemie', 'Anlauf', 'Kontinent', 'Griff', 'scheitern', 'Impfung', 'Pandemievertrag', 'WHO', 'WEF', 'BigPharma']\n"
     ]
    }
   ],
   "source": [
    "!snscrape --jsonl --max-results 100 twitter-hashtag plandemie > data/twitter/hashtag-plandemie.json\n",
    "\n",
    "\n",
    "with open(data_dir+\"/hashtag-plandemie.json\", \"r\") as fo:\n",
    "    tweets = fo.readlines()\n",
    "\n",
    "tweets = [json.loads(tweets[i]) for i in range(0, len(tweets))]\n",
    "\n",
    "\n",
    "print(\"loaded\", len(tweets), \"tweets\\n\")\n",
    "\n",
    "\n",
    "# print the first 5\n",
    "for i in range(min(5, len(tweets))):\n",
    "    print(tweets[i][\"date\"], tweets[i][\"rawContent\"], tweets[i][\"hashtags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8380fdfd-7d75-427c-8c83-34fdce53167a",
   "metadata": {},
   "source": [
    "All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e0de6-01b7-4183-a0a9-7af8029e0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a759d5-7093-44be-872f-6cd5e6fa35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!snscrape --jsonl twitter-hashtag plandemic > data/twitter/hashtag-plandemic.json\n",
    "\n",
    "\n",
    "with open(data_dir+\"/hashtag-plandemic.json\", \"r\") as fo:\n",
    "    tweets = fo.readlines()\n",
    "\n",
    "tweets = [json.loads(tweets[i]) for i in range(0, len(tweets))]\n",
    "\n",
    "\n",
    "print(\"loaded\", len(tweets), \"tweets\\n\")\n",
    "\n",
    "\n",
    "# print the first 5\n",
    "for i in range(min(5, len(tweets))):\n",
    "    print(tweets[i][\"date\"], tweets[i][\"rawContent\"], tweets[i][\"hashtags\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
