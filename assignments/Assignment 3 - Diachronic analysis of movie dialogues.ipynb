{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a717ea9a-d5ea-4d55-bea8-260a40e1076d",
   "metadata": {},
   "source": [
    "Assignment 3 - Diachronic analysis of movie dialogues\n",
    "===\n",
    "\n",
    "*Due: January 1 2023*\n",
    "\n",
    "In this assignment you will analyze how the expressionÂ of Hope in movie dialogues changed with time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c4f04c-b0ca-42ef-a96e-3839bcd32438",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2d76af2-250f-4b07-a889-b033fb948bca",
   "metadata": {},
   "source": [
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0a66a017-7a31-460c-a74a-2fff691e7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import itertools\n",
    "import nltk\n",
    "import regex as re\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "layout = widgets.Layout(width=\"auto\")\n",
    "lang = \"eng\"\n",
    "data_path = \"data/dialogs_preprocessed2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea52e22-c3ff-4f6d-a89b-12c6262efe35",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfe477-ef86-43ee-97f9-eb24de4683dd",
   "metadata": {},
   "source": [
    "### Functions for text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a65279c8-3780-4777-8ba8-e193159d62ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(input):\n",
    "    output = re.sub(r\"http\\S+\", \"\", input)\n",
    "    return output\n",
    "\n",
    "\n",
    "def fix_contraction(input):\n",
    "    output = contractions.fix(input)\n",
    "    return output\n",
    "\n",
    "\n",
    "def clean_non_alphanumeric(input):\n",
    "    output = re.sub(r\"[^a-zA-Z0-9]\", \" \", input)\n",
    "    return output\n",
    "\n",
    "\n",
    "def clean_tokenization(input):\n",
    "    output = nltk.word_tokenize(input)\n",
    "    return output\n",
    "\n",
    "\n",
    "def clean_stopwords(input):\n",
    "    output = [item for item in input if item not in stop_words]\n",
    "    return output\n",
    "\n",
    "\n",
    "def numbers_to_words(input):\n",
    "    output = []\n",
    "    for item in input:\n",
    "        if item.isnumeric() == True:\n",
    "            output += [p.number_to_words(item)]\n",
    "        else:\n",
    "            output += [item]\n",
    "    return output\n",
    "\n",
    "\n",
    "def clean_lowercase(input):\n",
    "    output = str(input).lower()\n",
    "    return output\n",
    "\n",
    "\n",
    "def clean_lemmatization(input):\n",
    "    output = [lemma.lemmatize(word=w, pos=\"v\") for w in input]\n",
    "    return output\n",
    "\n",
    "\n",
    "def clean_length(input):\n",
    "    output = [word for word in input if len(word) > 2]\n",
    "    return output\n",
    "\n",
    "\n",
    "def convert_to_string(input):\n",
    "    output = \" \".join(input)\n",
    "    return output\n",
    "\n",
    "\n",
    "def preprocessing(text, remove_stopwords=True):\n",
    "    \"\"\"\n",
    "    Preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    text = clean_url(text)\n",
    "    text = fix_contraction(text)\n",
    "    text = clean_non_alphanumeric(text)\n",
    "    text = clean_lowercase(text)\n",
    "    text = clean_tokenization(text)\n",
    "    text = numbers_to_words(text)\n",
    "    if remove_stopwords:\n",
    "        text = clean_stopwords(text)\n",
    "    text = clean_lemmatization(text)\n",
    "    text = clean_length(text)\n",
    "    text = convert_to_string(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1ed30-89b9-4f73-9d36-2b30a2115ecc",
   "metadata": {},
   "source": [
    "### Functions for wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "95f3c87e-4e4f-4ba1-8600-f0923cc2c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_list(seed_word, language):\n",
    "    \"\"\"\n",
    "    Takes in a single seed word and returns\n",
    "    a word list, the length and a list of synsets\n",
    "    \"\"\"\n",
    "\n",
    "    ## we create an empty list to store the final word list\n",
    "    list_of_lemmas = []\n",
    "    list_of_meanings = []\n",
    "\n",
    "    ## a function to add a word to a list\n",
    "    add_to_list = lambda list1, item1: list1.append(item1)\n",
    "\n",
    "    ## a function to return the hyponyms of a synset\n",
    "    hypos = lambda s: s.hyponyms()\n",
    "\n",
    "    ## wn.synset obtains the list of synonyms and meanings for that word, in different syntactic categories\n",
    "    meanings = wn.synsets(seed_word, pos=wn.NOUN + wn.VERB + wn.ADJ)\n",
    "\n",
    "    ## loop over set of meanings in synset\n",
    "    for meaning in meanings:\n",
    "\n",
    "        ## add synset, definition, and a list of all associated lemmas into the list_of_meanings\n",
    "        list_of_meanings += [\n",
    "            [\n",
    "                meaning,\n",
    "                meaning.definition(),\n",
    "                [lemma.name() for lemma in meaning.lemmas(language)],\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        ## append all synonyms (lemmas()) of that meaning to the list_of_lemmas\n",
    "        [\n",
    "            add_to_list(list_of_lemmas, lemma.name())\n",
    "            for lemma in meaning.lemmas(language)\n",
    "        ]\n",
    "\n",
    "        ## loop over the list of all possible hyponyms\n",
    "        for hyponym in meaning.closure(hypos):\n",
    "\n",
    "            ## add synsets, definition, and a list of all associated lemmas into the list_of_meanings\n",
    "            list_of_meanings += [\n",
    "                [\n",
    "                    hyponym,\n",
    "                    hyponym.definition(),\n",
    "                    [lemma.name() for lemma in hyponym.lemmas(language)],\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "            ## append all synonyms (lemmas()) of that hyponym to the list_of_lemmas\n",
    "            [\n",
    "                add_to_list(list_of_lemmas, lemma.name())\n",
    "                for lemma in hyponym.lemmas(language)\n",
    "            ]\n",
    "\n",
    "    ##eliminate list duplications by applying the set transformation\n",
    "    set_of_lemmas = [*set(list_of_lemmas)]\n",
    "\n",
    "    ## sort alphabetically\n",
    "    set_of_lemmas.sort()\n",
    "\n",
    "    ##length\n",
    "    length = len(set_of_lemmas)\n",
    "\n",
    "    return (set_of_lemmas, length, list_of_meanings)\n",
    "\n",
    "\n",
    "def prune_list(w, syn_list):\n",
    "    \"\"\"\n",
    "    Takes in a container checkbox widget,\n",
    "    a list with synsets and returns\n",
    "    a filtered word list.\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_meanings = list(\n",
    "        itertools.compress(syn_list, [widget.value for widget in w.children])\n",
    "    )\n",
    "\n",
    "    filtered_list = [word for lemmas in filtered_meanings for word in lemmas[2]]\n",
    "\n",
    "    filtered_list = sorted([*set(filtered_list)])\n",
    "\n",
    "    return filtered_list\n",
    "\n",
    "\n",
    "def expand_meanings(seed_words, language):\n",
    "    \"\"\"\n",
    "    Takes in a list of seed words and returns all synsets.\n",
    "    \"\"\"\n",
    "\n",
    "    list_meanings = []\n",
    "\n",
    "    for sw in seed_words:\n",
    "        _, _, meanings = generate_word_list(sw, language)\n",
    "        list_meanings += meanings\n",
    "\n",
    "    list_meanings.sort()\n",
    "\n",
    "    # groupby also eliminates duplications\n",
    "    list(list_meanings for list_meanings, _ in itertools.groupby(list_meanings))\n",
    "\n",
    "    return list_meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeca517-484d-4943-9e15-306782c8e44c",
   "metadata": {},
   "source": [
    "### Functions for vec2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "795a3782-5b77-4a6d-9f68-b7c16618b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def literary_words_list(root_folder):\n",
    "    \"\"\"\n",
    "\n",
    "    Function to select .txt files and store them as a list of paragraphs,\n",
    "    each a list of words, to use as input to the function WordVec.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_folder : a file path where .txt files are located\n",
    "    e.g. '~/home/user/text_analyses'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A list of the paragraphs, each paragraph a list of words\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    words_list = []\n",
    "\n",
    "    for path, subdirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if \".txt\" in file and \"model\" not in file:\n",
    "                # print(file)\n",
    "                name = os.path.join(path, file)\n",
    "                file_text = open(name, encoding=\"utf-8\").read()\n",
    "                text_list_paragraphs = file_text.split(\"\\n\")\n",
    "\n",
    "                for paragraph in text_list_paragraphs:\n",
    "                    paragraph = paragraph.replace(\"\\r\", \"\")\n",
    "                    list_of_sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "                    for sentence in list_of_sentences:\n",
    "                        words_list += [preprocessing(sentence)]\n",
    "\n",
    "    words_list = [w for w in words_list if w != \"\"]\n",
    "\n",
    "    return words_list\n",
    "\n",
    "\n",
    "def get_word2vec_list(word_list, model):\n",
    "    \"\"\"\n",
    "    Function to use word2vec to inquiry about the 10 most similar semantically\n",
    "    words to each seed word in word_list. Uses a pre-trained model to get the\n",
    "    most similar words within the text.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_word2vec_lists = []\n",
    "    for word in word_list:\n",
    "        try:\n",
    "\n",
    "            ##\n",
    "            list_vects = model.wv.most_similar([word], topn=10)\n",
    "\n",
    "            new_list = []\n",
    "            new_list += [word]\n",
    "            for item in list_vects:\n",
    "                word1 = item[0]\n",
    "                new_list += [word1]\n",
    "\n",
    "            # print(new_list)\n",
    "            # print('\\n')\n",
    "            list_of_word2vec_lists += [new_list]\n",
    "\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return list_of_word2vec_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b785f-9684-40e5-8889-ebf4445ed480",
   "metadata": {},
   "source": [
    "## 0. Expression of interest: Hope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "39dad581-fd7b-40e5-9fe6-3bb2bdbd18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_word = \"hope\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb5fae-a31f-4366-b5c0-28e022f84e5c",
   "metadata": {},
   "source": [
    "## 1. Control measure: Irritation\n",
    "\n",
    "Find control measure to contrast with \"Hope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8e82b40c-d2eb-42d4-8673-148e1a204b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_seed_word = \"irritation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6644d-9455-4e26-b1f1-d70891d552ce",
   "metadata": {},
   "source": [
    "## 2. Initial bags of seeds\n",
    "\n",
    "Find psychometric tools or equivalent to generate initial bags of seeds, for both Hope and the control condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6de92a90-55d3-4305-be65-b338ad64d84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of lemma list is 16\n"
     ]
    }
   ],
   "source": [
    "seed_list = generate_word_list(seed_word, lang)\n",
    "print(f\"Length of lemma list is {seed_list[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "60bdecc2-4306-4b51-b2b2-74c6adc1eee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of lemma list is 29\n"
     ]
    }
   ],
   "source": [
    "ctrl_seed_list = generate_word_list(control_seed_word, lang)\n",
    "print(f\"Length of lemma list is {ctrl_seed_list[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2957f916-3bd7-4849-857c-fbab3bb1b60f",
   "metadata": {},
   "source": [
    "## 3. Expand bag of seeds and get synonyms and hyponyms\n",
    "\n",
    "Use WordNet tools to expand your bag of seeds and get synonyms and hyponyms, and start excluding words with unrelated meanings using the filters available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7cb75-0a2d-45ac-9d64-8d4c8b357be8",
   "metadata": {},
   "source": [
    "### Hope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e4ac8433-af75-4810-a93d-47dbdfa67eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c4d52218f94997863146467f8a825c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Checkbox(value=True, description=\"[Synset('hope.n.01'), 'a specific instance of feeling hopefulâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selection_widget = widgets.VBox(\n",
    "    [\n",
    "        widgets.Checkbox(\n",
    "            value=True,\n",
    "            description=str(item),\n",
    "            disabled=False,\n",
    "            indent=False,\n",
    "            layout=layout,\n",
    "        )\n",
    "        for item in seed_list[2]\n",
    "    ]\n",
    ")\n",
    "\n",
    "selection_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ecf67084-2bdd-4881-b063-4aaa30a010ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['desire', 'encouragement', 'go_for', 'great_white_hope', 'hope', 'hopefulness', 'optimism', 'promise', 'rainbow', 'sanguineness', 'sanguinity', 'trust', 'white_hope']\n"
     ]
    }
   ],
   "source": [
    "seed_words = prune_list(selection_widget, seed_list[2])\n",
    "\n",
    "print(seed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88eead8-5666-4925-bcd2-a86b532af393",
   "metadata": {},
   "source": [
    "#### Extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b5d81585-685f-4079-8cc9-ddb87281d048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synsets = expand_meanings(seed_words, lang)\n",
    "len(synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c493ba2f-d55a-409d-b354-2ead1b402b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('abetment.n.01'),\n",
       "  'the verbal act of urging on',\n",
       "  ['abetment', 'abettal', 'instigation']],\n",
       " [Synset('accept.v.03'),\n",
       "  'give an affirmative reply to; respond favorably to',\n",
       "  ['accept', 'consent', 'go_for']],\n",
       " [Synset('accept.v.07'),\n",
       "  'tolerate or accommodate oneself to',\n",
       "  ['accept', 'live_with', 'swallow']]]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synsets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3137d9-0c48-4cca-8d7b-d8a846e612b9",
   "metadata": {},
   "source": [
    "### Irritation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d094686c-080c-4994-84f1-299a239047e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6866f7b65c4a37ba4398e8e33c71a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Checkbox(value=True, description=\"[Synset('irritation.n.01'), 'the psychological state of beingâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ctrl_selection_widget = widgets.VBox(\n",
    "    [\n",
    "        widgets.Checkbox(\n",
    "            value=True,\n",
    "            description=str(item),\n",
    "            disabled=False,\n",
    "            indent=False,\n",
    "            layout=layout,\n",
    "        )\n",
    "        for item in ctrl_seed_list[2]\n",
    "    ]\n",
    ")\n",
    "\n",
    "ctrl_selection_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "89196891-0a0f-4da5-8eef-7f3c11f0ef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aggravation', 'aggro', 'annoyance', 'annoying', 'botheration', 'bummer', 'discomfort', 'exacerbation', 'exasperation', 'excitation', 'huff', 'impatience', 'innervation', 'irritation', 'last_straw', 'miff', 'pinprick', 'pique', 'provocation', 'red_flag', 'restlessness', 'seeing_red', 'snit', 'soreness', 'taunt', 'taunting', 'temper', 'twit', 'vexation']\n"
     ]
    }
   ],
   "source": [
    "ctrl_seed_words = prune_list(ctrl_selection_widget, ctrl_seed_list[2])\n",
    "\n",
    "print(ctrl_seed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49b4fc-fd8a-470a-aab8-e919a5def6d2",
   "metadata": {},
   "source": [
    "#### Extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fa501637-8bcb-47bf-aaa5-1f579c8b59f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl_synsets = expand_meanings(ctrl_seed_words, lang)\n",
    "len(ctrl_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "40712a7c-4ea5-43c2-993a-71aa3eb536e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('abatable_nuisance.n.01'),\n",
       "  'a nuisance that can remedied (suppressed or extinguished or rendered harmless)',\n",
       "  ['abatable_nuisance']],\n",
       " [Synset('abatable_nuisance.n.01'),\n",
       "  'a nuisance that can remedied (suppressed or extinguished or rendered harmless)',\n",
       "  ['abatable_nuisance']],\n",
       " [Synset('aggravation.n.01'),\n",
       "  'an exasperated feeling of annoyance',\n",
       "  ['aggravation', 'exasperation']]]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl_synsets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b376a7-4250-4b7a-ad75-5b7e5be69341",
   "metadata": {},
   "source": [
    "## 4. Train semantic vector space model\n",
    "\n",
    "Train a semantic vector space model using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8cad7d78-4dcb-46ee-8025-6771c7f5c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_input = literary_words_list(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61def98a-c7b0-467c-84c7-63f6509ed524",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">Is this following step necessary? Otherwise it wouldn't work. If yes, why not including it into the function literary_words_list?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "549a4f51-e4d5-4b65-949f-fe12df1b4b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each sentence to list -> necessary?\n",
    "word2vec_input_list = [s.split() for s in word2vec_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c8789cfc-7e34-4f3f-a3db-43211ead41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_output = Word2Vec(word2vec_input_list, min_count=1)\n",
    "\n",
    "## Save vector space\n",
    "word2vec_output.save(\"w2v_model.txt\")\n",
    "model = Word2Vec.load(\"w2v_model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b6c7d-f48b-4f92-97b2-d4312ef9aaeb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">Should the output look like this? A list of lists?<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c91d4a9b-a958-4349-8451-278227f1925b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['desire',\n",
       "  'belief',\n",
       "  'guilt',\n",
       "  'wisdom',\n",
       "  'destiny',\n",
       "  'painful',\n",
       "  'nature',\n",
       "  'ability',\n",
       "  'passion',\n",
       "  'mortal',\n",
       "  'fate'],\n",
       " ['encouragement',\n",
       "  'placate',\n",
       "  'constable',\n",
       "  'knowi',\n",
       "  'caution',\n",
       "  'vagina',\n",
       "  'lucian',\n",
       "  'rant',\n",
       "  'resolute',\n",
       "  'bumstead',\n",
       "  'astound']]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_vector_space = get_word2vec_list(filtered_list, model)\n",
    "check_vector_space[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3485b-43b7-48f4-b842-088287935746",
   "metadata": {},
   "source": [
    "## 5. Semantic clouds\n",
    "\n",
    "Use your vector space model to find out the semantic clouds of each word in your bag of seeds, and select only the words with semantically meaningful clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddca6110-2e4e-4b15-89cf-e79c847672b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a44741-d1a4-4dd3-af12-7a0850f5695f",
   "metadata": {},
   "source": [
    "## 6. Final bag of rods and word frequencies\n",
    "\n",
    "Take the final Bag of Words, prune it further if necessary, and use the second script to calculate word frequencies (don't forget to upload the text files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3640fc8-cb08-4011-ac7c-d97721423c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775c265-894e-47d7-9b78-71d19c71e9ac",
   "metadata": {},
   "source": [
    "## 7. Relevant ratio, visualization of time series\n",
    "\n",
    "Calculate your psychological relevant ratio, build a dataframe and plot a time series using Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3537078-024a-45cc-a700-92f99a65f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
