{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a717ea9a-d5ea-4d55-bea8-260a40e1076d",
   "metadata": {},
   "source": [
    "Assignment 3 - Diachronic analysis of movie dialogues\n",
    "===\n",
    "\n",
    "*Due: January 1 2023*\n",
    "\n",
    "In this assignment you will analyze how the expressionÂ of Hope in movie dialogues changed with time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c4f04c-b0ca-42ef-a96e-3839bcd32438",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2d76af2-250f-4b07-a889-b033fb948bca",
   "metadata": {},
   "source": [
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "0a66a017-7a31-460c-a74a-2fff691e7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import itertools\n",
    "import nltk\n",
    "import regex as re\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import ast\n",
    "from glob import glob\n",
    "import collections\n",
    "\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "layout = widgets.Layout(width=\"auto\")\n",
    "lang = \"eng\"\n",
    "data_path = \"data/dialogs_preprocessed2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea52e22-c3ff-4f6d-a89b-12c6262efe35",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfe477-ef86-43ee-97f9-eb24de4683dd",
   "metadata": {},
   "source": [
    "### Functions for text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a65279c8-3780-4777-8ba8-e193159d62ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(input):\n",
    "    output = re.sub(r\"http\\S+\", \"\", input)\n",
    "    return output\n",
    "\n",
    "\n",
    "def fix_contraction(input):\n",
    "    output = contractions.fix(input)\n",
    "    return output\n",
    "\n",
    "\n",
    "def clean_non_alphanumeric(input):\n",
    "    output = re.sub(r\"[^a-zA-Z0-9]\", \" \", input)\n",
    "    return output\n",
    "\n",
    "\n",
    "def clean_tokenization(input):\n",
    "    output = nltk.word_tokenize(input)\n",
    "    return output\n",
    "\n",
    "\n",
    "def clean_stopwords(input):\n",
    "    output = [item for item in input if item not in stop_words]\n",
    "    return output\n",
    "\n",
    "\n",
    "def numbers_to_words(input):\n",
    "    output = []\n",
    "    for item in input:\n",
    "        if item.isnumeric() == True:\n",
    "            output += [p.number_to_words(item)]\n",
    "        else:\n",
    "            output += [item]\n",
    "    return output\n",
    "\n",
    "\n",
    "def clean_lowercase(input):\n",
    "    output = str(input).lower()\n",
    "    return output\n",
    "\n",
    "\n",
    "def clean_lemmatization(input):\n",
    "    output = [lemma.lemmatize(word=w, pos=\"v\") for w in input]\n",
    "    return output\n",
    "\n",
    "\n",
    "def clean_length(input):\n",
    "    output = [word for word in input if len(word) > 2]\n",
    "    return output\n",
    "\n",
    "\n",
    "def convert_to_string(input):\n",
    "    output = \" \".join(input)\n",
    "    return output\n",
    "\n",
    "\n",
    "def preprocessing(text, remove_stopwords=True):\n",
    "    \"\"\"\n",
    "    Preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    text = clean_url(text)\n",
    "    text = fix_contraction(text)\n",
    "    text = clean_non_alphanumeric(text)\n",
    "    text = clean_lowercase(text)\n",
    "    text = clean_tokenization(text)\n",
    "    text = numbers_to_words(text)\n",
    "    if remove_stopwords:\n",
    "        text = clean_stopwords(text)\n",
    "    text = clean_lemmatization(text)\n",
    "    text = clean_length(text)\n",
    "    text = convert_to_string(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1ed30-89b9-4f73-9d36-2b30a2115ecc",
   "metadata": {},
   "source": [
    "### Functions for wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "95f3c87e-4e4f-4ba1-8600-f0923cc2c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_list(seed_word, language):\n",
    "    \"\"\"\n",
    "    Takes in a single seed word and returns\n",
    "    a word list, the length and a list of synsets\n",
    "    \"\"\"\n",
    "\n",
    "    ## we create an empty list to store the final word list\n",
    "    list_of_lemmas = []\n",
    "    list_of_meanings = []\n",
    "\n",
    "    ## a function to add a word to a list\n",
    "    add_to_list = lambda list1, item1: list1.append(item1)\n",
    "\n",
    "    ## a function to return the hyponyms of a synset\n",
    "    hypos = lambda s: s.hyponyms()\n",
    "\n",
    "    ## wn.synset obtains the list of synonyms and meanings for that word, in different syntactic categories\n",
    "    meanings = wn.synsets(seed_word, pos=wn.NOUN + wn.VERB + wn.ADJ)\n",
    "\n",
    "    ## loop over set of meanings in synset\n",
    "    for meaning in meanings:\n",
    "\n",
    "        ## add synset, definition, and a list of all associated lemmas into the list_of_meanings\n",
    "        list_of_meanings += [\n",
    "            [\n",
    "                meaning,\n",
    "                meaning.definition(),\n",
    "                [lemma.name() for lemma in meaning.lemmas(language)],\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        ## append all synonyms (lemmas()) of that meaning to the list_of_lemmas\n",
    "        [\n",
    "            add_to_list(list_of_lemmas, lemma.name())\n",
    "            for lemma in meaning.lemmas(language)\n",
    "        ]\n",
    "\n",
    "        ## loop over the list of all possible hyponyms\n",
    "        for hyponym in meaning.closure(hypos):\n",
    "\n",
    "            ## add synsets, definition, and a list of all associated lemmas into the list_of_meanings\n",
    "            list_of_meanings += [\n",
    "                [\n",
    "                    hyponym,\n",
    "                    hyponym.definition(),\n",
    "                    [lemma.name() for lemma in hyponym.lemmas(language)],\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "            ## append all synonyms (lemmas()) of that hyponym to the list_of_lemmas\n",
    "            [\n",
    "                add_to_list(list_of_lemmas, lemma.name())\n",
    "                for lemma in hyponym.lemmas(language)\n",
    "            ]\n",
    "\n",
    "    ##eliminate list duplications by applying the set transformation\n",
    "    set_of_lemmas = [*set(list_of_lemmas)]\n",
    "\n",
    "    ## sort alphabetically\n",
    "    set_of_lemmas.sort()\n",
    "\n",
    "    ##length\n",
    "    length = len(set_of_lemmas)\n",
    "\n",
    "    return (set_of_lemmas, length, list_of_meanings)\n",
    "\n",
    "\n",
    "def prune_list(w, syn_list):\n",
    "    \"\"\"\n",
    "    Takes in a container checkbox widget,\n",
    "    a list with synsets and returns\n",
    "    a filtered word list.\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_meanings = list(\n",
    "        itertools.compress(syn_list, [widget.value for widget in w.children])\n",
    "    )\n",
    "\n",
    "    filtered_list = [word for lemmas in filtered_meanings for word in lemmas[2]]\n",
    "\n",
    "    filtered_list = sorted([*set(filtered_list)])\n",
    "\n",
    "    return filtered_list\n",
    "\n",
    "\n",
    "def expand_meanings(seed_words, language):\n",
    "    \"\"\"\n",
    "    Takes in a list of seed words and returns all synsets.\n",
    "    \"\"\"\n",
    "\n",
    "    list_meanings = []\n",
    "\n",
    "    for sw in seed_words:\n",
    "        _, _, meanings = generate_word_list(sw, language)\n",
    "        list_meanings += meanings\n",
    "\n",
    "    list_meanings.sort()\n",
    "\n",
    "    # groupby also eliminates duplications\n",
    "    list(list_meanings for list_meanings, _ in itertools.groupby(list_meanings))\n",
    "\n",
    "    return list_meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeca517-484d-4943-9e15-306782c8e44c",
   "metadata": {},
   "source": [
    "### Functions for vec2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "795a3782-5b77-4a6d-9f68-b7c16618b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2vec_input(root_folder):\n",
    "    \"\"\"\n",
    "\n",
    "    Function to select .txt files and store them as a list of paragraphs,\n",
    "    each a list of words, to use as input to the function WordVec.\n",
    "\n",
    "    This function was originally named \"literary_words_list\".\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_folder : a file path where .txt files are located\n",
    "    e.g. '~/home/user/text_analyses'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A list of the paragraphs, each paragraph a list of words\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    word2vec_input = []\n",
    "\n",
    "    for path, subdirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if \".txt\" in file and \"model\" not in file:\n",
    "                # print(file)\n",
    "                name = os.path.join(path, file)\n",
    "                file_text = open(name, encoding=\"utf-8\").read()\n",
    "                text_list_paragraphs = file_text.split(\"\\n\")\n",
    "\n",
    "                for paragraph in text_list_paragraphs:\n",
    "                    paragraph = paragraph.replace(\"\\r\", \"\")\n",
    "                    list_of_sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "                    for sentence in list_of_sentences:\n",
    "                        word2vec_input += [preprocessing(sentence)]\n",
    "\n",
    "    # remove empty strings and split sentences into lists\n",
    "    word2vec_input = [s.split() for s in word2vec_input if s != \"\"]\n",
    "\n",
    "    return word2vec_input\n",
    "\n",
    "\n",
    "def get_word2vec_list(word_list, model):\n",
    "    \"\"\"\n",
    "    Function to use word2vec to inquiry about the 10 most similar semantically\n",
    "    words to each seed word in word_list. Uses a pre-trained model to get the\n",
    "    most similar words within the text.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_word2vec_lists = []\n",
    "    for word in word_list:\n",
    "        try:\n",
    "\n",
    "            ##\n",
    "            list_vects = model.wv.most_similar([word], topn=10)\n",
    "\n",
    "            new_list = []\n",
    "            new_list += [word]\n",
    "            for item in list_vects:\n",
    "                word1 = item[0]\n",
    "                new_list += [word1]\n",
    "\n",
    "            # print(new_list)\n",
    "            # print('\\n')\n",
    "            list_of_word2vec_lists += [new_list]\n",
    "\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return list_of_word2vec_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414557b0-1601-42d4-be81-ea9541ecbf48",
   "metadata": {},
   "source": [
    "### Functions for word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "4bb3c6fd-ac13-48f0-894a-536c72eea9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequencies(list_string, list_of_categories):\n",
    "    \"\"\"\n",
    "    Function to calculate word frequencies.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    global apply_counter1\n",
    "\n",
    "    tokenized_text = ast.literal_eval(list_string)\n",
    "    word_counter = collections.Counter(tokenized_text)\n",
    "    total_words = len(tokenized_text)\n",
    "\n",
    "    list_of_category_frequencies = []\n",
    "    for category in list_of_categories:\n",
    "        category_count = 0.0\n",
    "\n",
    "        for word in category:\n",
    "            category_count += int(word_counter[word])\n",
    "\n",
    "        if total_words != 0 and category_count > 0:\n",
    "            category_frequency = category_count / total_words\n",
    "        else:\n",
    "            category_frequency = float(\"nan\")\n",
    "        list_of_category_frequencies += [category_frequency]\n",
    "\n",
    "    apply_counter1 += 1\n",
    "\n",
    "    return list_of_category_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b785f-9684-40e5-8889-ebf4445ed480",
   "metadata": {},
   "source": [
    "## 0. Expression of interest: Hope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "39dad581-fd7b-40e5-9fe6-3bb2bdbd18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_word = \"hope\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb5fae-a31f-4366-b5c0-28e022f84e5c",
   "metadata": {},
   "source": [
    "## 1. Control measure: Irritation\n",
    "\n",
    "Find control measure to contrast with \"Hope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8e82b40c-d2eb-42d4-8673-148e1a204b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_seed_word = \"irritation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6644d-9455-4e26-b1f1-d70891d552ce",
   "metadata": {},
   "source": [
    "## 2. Initial bags of seeds\n",
    "\n",
    "Find psychometric tools or equivalent to generate initial bags of seeds, for both Hope and the control condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6de92a90-55d3-4305-be65-b338ad64d84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of lemma list is 16\n"
     ]
    }
   ],
   "source": [
    "seed_list = generate_word_list(seed_word, lang)\n",
    "print(f\"Length of lemma list is {seed_list[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "60bdecc2-4306-4b51-b2b2-74c6adc1eee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of lemma list is 29\n"
     ]
    }
   ],
   "source": [
    "ctrl_seed_list = generate_word_list(control_seed_word, lang)\n",
    "print(f\"Length of lemma list is {ctrl_seed_list[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2957f916-3bd7-4849-857c-fbab3bb1b60f",
   "metadata": {},
   "source": [
    "## 3. Expand bag of seeds and get synonyms and hyponyms\n",
    "\n",
    "Use WordNet tools to expand your bag of seeds and get synonyms and hyponyms, and start excluding words with unrelated meanings using the filters available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7cb75-0a2d-45ac-9d64-8d4c8b357be8",
   "metadata": {},
   "source": [
    "### Hope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e4ac8433-af75-4810-a93d-47dbdfa67eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a21ff0a02e4af881ee9a9eb708dea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Checkbox(value=True, description=\"[Synset('hope.n.01'), 'a specific instance of feeling hopefulâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selection_widget = widgets.VBox(\n",
    "    [\n",
    "        widgets.Checkbox(\n",
    "            value=True,\n",
    "            description=str(item),\n",
    "            disabled=False,\n",
    "            indent=False,\n",
    "            layout=layout,\n",
    "        )\n",
    "        for item in seed_list[2]\n",
    "    ]\n",
    ")\n",
    "\n",
    "selection_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ecf67084-2bdd-4881-b063-4aaa30a010ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['desire', 'encouragement', 'go_for', 'great_white_hope', 'hope', 'hopefulness', 'optimism', 'promise', 'rainbow', 'sanguineness', 'sanguinity', 'trust', 'white_hope']\n"
     ]
    }
   ],
   "source": [
    "seed_words = prune_list(selection_widget, seed_list[2])\n",
    "\n",
    "print(seed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88eead8-5666-4925-bcd2-a86b532af393",
   "metadata": {},
   "source": [
    "#### Extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b5d81585-685f-4079-8cc9-ddb87281d048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synsets = expand_meanings(seed_words, lang)\n",
    "len(synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c493ba2f-d55a-409d-b354-2ead1b402b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('abetment.n.01'),\n",
       "  'the verbal act of urging on',\n",
       "  ['abetment', 'abettal', 'instigation']],\n",
       " [Synset('accept.v.03'),\n",
       "  'give an affirmative reply to; respond favorably to',\n",
       "  ['accept', 'consent', 'go_for']],\n",
       " [Synset('accept.v.07'),\n",
       "  'tolerate or accommodate oneself to',\n",
       "  ['accept', 'live_with', 'swallow']]]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synsets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3137d9-0c48-4cca-8d7b-d8a846e612b9",
   "metadata": {},
   "source": [
    "### Irritation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d094686c-080c-4994-84f1-299a239047e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2891d5a90206409f9e040248233dcc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Checkbox(value=True, description=\"[Synset('irritation.n.01'), 'the psychological state of beingâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ctrl_selection_widget = widgets.VBox(\n",
    "    [\n",
    "        widgets.Checkbox(\n",
    "            value=True,\n",
    "            description=str(item),\n",
    "            disabled=False,\n",
    "            indent=False,\n",
    "            layout=layout,\n",
    "        )\n",
    "        for item in ctrl_seed_list[2]\n",
    "    ]\n",
    ")\n",
    "\n",
    "ctrl_selection_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "89196891-0a0f-4da5-8eef-7f3c11f0ef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aggravation', 'aggro', 'annoyance', 'annoying', 'botheration', 'bummer', 'discomfort', 'exacerbation', 'exasperation', 'excitation', 'huff', 'impatience', 'innervation', 'irritation', 'last_straw', 'miff', 'pinprick', 'pique', 'provocation', 'red_flag', 'restlessness', 'seeing_red', 'snit', 'soreness', 'taunt', 'taunting', 'temper', 'twit', 'vexation']\n"
     ]
    }
   ],
   "source": [
    "ctrl_seed_words = prune_list(ctrl_selection_widget, ctrl_seed_list[2])\n",
    "\n",
    "print(ctrl_seed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49b4fc-fd8a-470a-aab8-e919a5def6d2",
   "metadata": {},
   "source": [
    "#### Extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fa501637-8bcb-47bf-aaa5-1f579c8b59f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl_synsets = expand_meanings(ctrl_seed_words, lang)\n",
    "len(ctrl_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "40712a7c-4ea5-43c2-993a-71aa3eb536e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('abatable_nuisance.n.01'),\n",
       "  'a nuisance that can remedied (suppressed or extinguished or rendered harmless)',\n",
       "  ['abatable_nuisance']],\n",
       " [Synset('abatable_nuisance.n.01'),\n",
       "  'a nuisance that can remedied (suppressed or extinguished or rendered harmless)',\n",
       "  ['abatable_nuisance']],\n",
       " [Synset('aggravation.n.01'),\n",
       "  'an exasperated feeling of annoyance',\n",
       "  ['aggravation', 'exasperation']]]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl_synsets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b376a7-4250-4b7a-ad75-5b7e5be69341",
   "metadata": {},
   "source": [
    "## 4. Train semantic vector space model\n",
    "\n",
    "Train a semantic vector space model using word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3f13b-4758-4cbc-92be-a865102d3a76",
   "metadata": {},
   "source": [
    "### Create input for vec2word from folder with textfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8cad7d78-4dcb-46ee-8025-6771c7f5c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_input = create_word2vec_input(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300a68f5-c10b-4e94-a5c9-8733dea1bee4",
   "metadata": {},
   "source": [
    "### Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c8789cfc-7e34-4f3f-a3db-43211ead41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_output = Word2Vec(word2vec_input_list, min_count=1)\n",
    "\n",
    "## Save vector space\n",
    "word2vec_output.save(\"data/w2v_model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa57503-f4ed-46cb-9d4d-19e104643ef2",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8932d3fb-27ae-4bdf-adf9-8bc30c6a9572",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"data/w2v_model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3485b-43b7-48f4-b842-088287935746",
   "metadata": {},
   "source": [
    "## 5. Semantic clouds\n",
    "\n",
    "Use your vector space model to find out the semantic clouds of each word in your bag of seeds, and select only the words with semantically meaningful clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec2d758-b848-4562-909c-1cd052d155ef",
   "metadata": {},
   "source": [
    "### Hope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ddca6110-2e4e-4b15-89cf-e79c847672b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_space = get_word2vec_list(filtered_list, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7afb0ac0-34cd-4e48-b010-3d8f93d8328f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hopefulness'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "354b8c52-cf80-4da1-812a-e8be9e480b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hopefulness',\n",
       " 'sonofabitch',\n",
       " 'durn',\n",
       " 'blighter',\n",
       " 'cunt',\n",
       " 'buzzard',\n",
       " 'motherfucker',\n",
       " 'sonovabitch',\n",
       " 'faggot',\n",
       " 'cocksucker',\n",
       " 'puttin']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_space[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c0a740-8aad-4868-be0e-32252b848037",
   "metadata": {},
   "source": [
    "### Irritation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "cd555758-543f-4f6c-9d14-7f3fc59a64b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_vector_space = get_word2vec_list(ctrl_filtered_list, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "dce1b321-88de-42d8-9f88-4f451b5817d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bummer'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl_filtered_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e3f6a4a8-ca47-4578-90a3-c059beb8679b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bummer',\n",
       " 'hangover',\n",
       " 'stinker',\n",
       " 'weenie',\n",
       " 'fruity',\n",
       " 'fixer',\n",
       " 'cheerleader',\n",
       " 'libel',\n",
       " 'disorientation',\n",
       " 'schoolgirl',\n",
       " 'turf']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl_vector_space[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a44741-d1a4-4dd3-af12-7a0850f5695f",
   "metadata": {},
   "source": [
    "## 6. Final bag of words and word frequencies\n",
    "\n",
    "Take the final Bag of Words, prune it further if necessary, and use the second script to calculate word frequencies (don't forget to upload the text files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d733b78-7776-4ebb-9db3-6e324260f3fa",
   "metadata": {},
   "source": [
    "### Hope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f3640fc8-cb08-4011-ac7c-d97721423c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095ccd2e7f9b44708b70d6cf26e738b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Checkbox(value=True, description=\"['desire', 'guilt', 'wisdom', 'belief', 'destiny', 'fate', 'mâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layout = widgets.Layout(width=\"auto\")\n",
    "\n",
    "selection_widget_2 = widgets.VBox(\n",
    "    [\n",
    "        widgets.Checkbox(\n",
    "            value=True,\n",
    "            description=str(item),\n",
    "            disabled=False,\n",
    "            indent=False,\n",
    "            layout=layout,\n",
    "        )\n",
    "        for item in vector_space\n",
    "    ]\n",
    ")\n",
    "\n",
    "selection_widget_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff098d1d-ddc6-497a-9340-770da167cf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_word2vec = list(\n",
    "    itertools.compress(\n",
    "        vector_space, [widget.value for widget in selection_widget_2.children]\n",
    "    )\n",
    ")\n",
    "\n",
    "## now we extract just the words\n",
    "flat_list = [word for lists in filtered_word2vec for word in lists]\n",
    "\n",
    "## eliminate duplications, and sort alphabetically\n",
    "flat_list = sorted([*set(flat_list)])\n",
    "\n",
    "len(flat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a48509-016d-4066-93a6-df1098de102e",
   "metadata": {},
   "source": [
    "And a final check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3e728748-d250-4ce9-af81-1d0627ca594f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5fec19e21746a098c47f61af3a7bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Checkbox(value=True, description='ability', indent=False, layout=Layout(width='auto')), Checkboâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selection_widget_3 = widgets.VBox(\n",
    "    [\n",
    "        widgets.Checkbox(\n",
    "            value=True,\n",
    "            description=str(item),\n",
    "            disabled=False,\n",
    "            indent=False,\n",
    "            layout=layout,\n",
    "        )\n",
    "        for item in flat_list\n",
    "    ]\n",
    ")\n",
    "\n",
    "selection_widget_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "969e9ae6-c270-4129-b4a0-c1c07059ed07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list = list(\n",
    "    itertools.compress(\n",
    "        flat_list, [widget.value for widget in selection_widget_3.children]\n",
    "    )\n",
    ")\n",
    "\n",
    "len(final_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5bb6a8-db43-4bd1-bd03-07f89000334d",
   "metadata": {},
   "source": [
    "### Irritation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "358ba04c-c358-4310-afd2-544655e3f7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6146d3880e4b4a2995642d4f37606010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Checkbox(value=True, description=\"['aggravation', 'assurance', 'leeway', 'distraction', 'insighâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ctrl_selection_widget_2 = widgets.VBox(\n",
    "    [\n",
    "        widgets.Checkbox(\n",
    "            value=True,\n",
    "            description=str(item),\n",
    "            disabled=False,\n",
    "            indent=False,\n",
    "            layout=layout,\n",
    "        )\n",
    "        for item in ctrl_vector_space\n",
    "    ]\n",
    ")\n",
    "\n",
    "ctrl_selection_widget_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c87552b2-bc9e-4a34-a318-69b426f85521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl_filtered_word2vec = list(\n",
    "    itertools.compress(\n",
    "        ctrl_vector_space, [widget.value for widget in ctrl_selection_widget_2.children]\n",
    "    )\n",
    ")\n",
    "\n",
    "## now we extract just the words\n",
    "ctrl_flat_list = [word for lists in ctrl_filtered_word2vec for word in lists]\n",
    "\n",
    "## eliminate duplications, and sort alphabetically\n",
    "ctrl_flat_list = sorted([*set(ctrl_flat_list)])\n",
    "\n",
    "len(ctrl_flat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0bf37d-2e84-4ef2-8469-32e1f2ba5ffd",
   "metadata": {},
   "source": [
    "And a final check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e23efab7-cc20-4242-af75-9a1d9ab9179c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd441ab76f59434abacca111b1c6d600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Checkbox(value=True, description='adamant', indent=False, layout=Layout(width='auto')), Checkboâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ctrl_selection_widget_3 = widgets.VBox(\n",
    "    [\n",
    "        widgets.Checkbox(\n",
    "            value=True,\n",
    "            description=str(item),\n",
    "            disabled=False,\n",
    "            indent=False,\n",
    "            layout=layout,\n",
    "        )\n",
    "        for item in ctrl_flat_list\n",
    "    ]\n",
    ")\n",
    "\n",
    "ctrl_selection_widget_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "af28595a-6d15-4956-b5ec-b99bf5c2be1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl_final_list = list(\n",
    "    itertools.compress(\n",
    "        ctrl_flat_list, [widget.value for widget in ctrl_selection_widget_3.children]\n",
    "    )\n",
    ")\n",
    "\n",
    "len(ctrl_final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2125d1bf-77b0-4b6e-9ba1-9c2a1f4ab623",
   "metadata": {},
   "outputs": [],
   "source": [
    "hope = final_list\n",
    "irritation = ctrl_final_list\n",
    "\n",
    "list_of_categories = [hope, irritation]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0ac76-fa8b-44c2-9650-b732098ed51a",
   "metadata": {},
   "source": [
    "### Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5bf64f-3900-444c-bd41-4c7e0b8925e6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">Is this correct? I was not sure what does the input need to look like. And from where does it need to come from? A string that looks like a list of strings???</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "306242fc-800c-44a5-a7f8-76d54917a90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>donniebrasco</td>\n",
       "      <td>data/dialogs_preprocessed2/donniebrasco_dialog...</td>\n",
       "      <td>['paul', 'attanasio', 'base', 'book', 'donnie'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shiningthe</td>\n",
       "      <td>data/dialogs_preprocessed2/shiningthe_dialog.txt</td>\n",
       "      <td>['post', 'production', 'script', 'july', 'get'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>idesofmarchthe</td>\n",
       "      <td>data/dialogs_preprocessed2/idesofmarchthe_dial...</td>\n",
       "      <td>['write', 'george', 'clooney', 'grant', 'heslo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hangoverthe</td>\n",
       "      <td>data/dialogs_preprocessed2/hangoverthe_dialog.txt</td>\n",
       "      <td>['write', 'jon', 'lucas', 'scott', 'moore', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bringingoutthedead</td>\n",
       "      <td>data/dialogs_preprocessed2/bringingoutthedead_...</td>\n",
       "      <td>['first', 'draft', 'paul', 'schrader', 'novel'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                movie                                               path  \\\n",
       "0        donniebrasco  data/dialogs_preprocessed2/donniebrasco_dialog...   \n",
       "1          shiningthe   data/dialogs_preprocessed2/shiningthe_dialog.txt   \n",
       "2      idesofmarchthe  data/dialogs_preprocessed2/idesofmarchthe_dial...   \n",
       "3         hangoverthe  data/dialogs_preprocessed2/hangoverthe_dialog.txt   \n",
       "4  bringingoutthedead  data/dialogs_preprocessed2/bringingoutthedead_...   \n",
       "\n",
       "                                                text  \n",
       "0  ['paul', 'attanasio', 'base', 'book', 'donnie'...  \n",
       "1  ['post', 'production', 'script', 'july', 'get'...  \n",
       "2  ['write', 'george', 'clooney', 'grant', 'heslo...  \n",
       "3  ['write', 'jon', 'lucas', 'scott', 'moore', 's...  \n",
       "4  ['first', 'draft', 'paul', 'schrader', 'novel'...  "
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(glob(data_path + \"/*.txt\"), columns=[\"movie\"])\n",
    "df[\"path\"] = df[\"movie\"]\n",
    "df[\"movie\"] = (\n",
    "    df[\"movie\"]\n",
    "    .str.replace(\"^\" + data_path, \"\", regex=True)\n",
    "    .str.replace(\"_dialog.txt$\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "\n",
    "df[\"text\"] = df[\"path\"].apply(lambda path: open(path, encoding=\"utf-8\").read())\n",
    "df[\"text\"] = df[\"text\"].apply(lambda raw_txt: preprocessing(raw_txt))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda txt: str(txt.split(\" \")))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "268ecb5f-b78e-4576-b243-7ef43e47f6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>hope</th>\n",
       "      <th>irritation</th>\n",
       "      <th>ratio_hope_irritation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>donniebrasco</td>\n",
       "      <td>data/dialogs_preprocessed2/donniebrasco_dialog...</td>\n",
       "      <td>['paul', 'attanasio', 'base', 'book', 'donnie'...</td>\n",
       "      <td>0.002886</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.724138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shiningthe</td>\n",
       "      <td>data/dialogs_preprocessed2/shiningthe_dialog.txt</td>\n",
       "      <td>['post', 'production', 'script', 'july', 'get'...</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.931034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>idesofmarchthe</td>\n",
       "      <td>data/dialogs_preprocessed2/idesofmarchthe_dial...</td>\n",
       "      <td>['write', 'george', 'clooney', 'grant', 'heslo...</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hangoverthe</td>\n",
       "      <td>data/dialogs_preprocessed2/hangoverthe_dialog.txt</td>\n",
       "      <td>['write', 'jon', 'lucas', 'scott', 'moore', 's...</td>\n",
       "      <td>0.004562</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bringingoutthedead</td>\n",
       "      <td>data/dialogs_preprocessed2/bringingoutthedead_...</td>\n",
       "      <td>['first', 'draft', 'paul', 'schrader', 'novel'...</td>\n",
       "      <td>0.002922</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                movie                                               path  \\\n",
       "0        donniebrasco  data/dialogs_preprocessed2/donniebrasco_dialog...   \n",
       "1          shiningthe   data/dialogs_preprocessed2/shiningthe_dialog.txt   \n",
       "2      idesofmarchthe  data/dialogs_preprocessed2/idesofmarchthe_dial...   \n",
       "3         hangoverthe  data/dialogs_preprocessed2/hangoverthe_dialog.txt   \n",
       "4  bringingoutthedead  data/dialogs_preprocessed2/bringingoutthedead_...   \n",
       "\n",
       "                                                text      hope  irritation  \\\n",
       "0  ['paul', 'attanasio', 'base', 'book', 'donnie'...  0.002886    0.000462   \n",
       "1  ['post', 'production', 'script', 'july', 'get'...  0.007365    0.000263   \n",
       "2  ['write', 'george', 'clooney', 'grant', 'heslo...  0.004899    0.000639   \n",
       "3  ['write', 'jon', 'lucas', 'scott', 'moore', 's...  0.004562    0.000480   \n",
       "4  ['first', 'draft', 'paul', 'schrader', 'novel'...  0.002922    0.000674   \n",
       "\n",
       "   ratio_hope_irritation  \n",
       "0               0.724138  \n",
       "1               0.931034  \n",
       "2               0.769231  \n",
       "3               0.809524  \n",
       "4               0.625000  "
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_counter1 = 0\n",
    "\n",
    "df[\"frequencies\"] = df[\"text\"].apply(\n",
    "    compute_frequencies, list_of_categories=list_of_categories\n",
    ")\n",
    "\n",
    "# Here we distribute the list of frequencies of different categories to different columns\n",
    "\n",
    "# first we create a list for each category ...\n",
    "hope2 = []\n",
    "irritation2 = []\n",
    "list_of_categories2 = [hope2, irritation2]\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    list_of_frequencies = row[\"frequencies\"]\n",
    "\n",
    "    for category, frequency in zip(list_of_categories2, list_of_frequencies):\n",
    "        category += [frequency]\n",
    "\n",
    "# ... and then we write each list to a column\n",
    "list_of_category_names = [\"hope\", \"irritation\"]\n",
    "\n",
    "for catName, catFreqs in zip(list_of_category_names, list_of_categories2):\n",
    "    df[catName] = catFreqs\n",
    "\n",
    "del df[\"frequencies\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faaeb67-69f2-4103-a65e-566c18ce4c88",
   "metadata": {},
   "source": [
    "Save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "a5176e5b-4c8e-4808-bcc5-0e5a06f7a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/hope_irritation_frequencies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775c265-894e-47d7-9b78-71d19c71e9ac",
   "metadata": {},
   "source": [
    "## 7. Relevant ratio, visualization of time series\n",
    "\n",
    "Calculate your psychological relevant ratio, build a dataframe and plot a time series using Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778c672-44d3-4cdf-bb25-a8db6ce75b22",
   "metadata": {},
   "source": [
    "### Ratio hope vs. irritation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e886bad-d477-420d-8520-2170bb9101bd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">What kind of ratio is this (relative difference)?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "d3537078-024a-45cc-a700-92f99a65f743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>hope</th>\n",
       "      <th>irritation</th>\n",
       "      <th>ratio_hope_irritation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>donniebrasco</td>\n",
       "      <td>data/dialogs_preprocessed2/donniebrasco_dialog...</td>\n",
       "      <td>['paul', 'attanasio', 'base', 'book', 'donnie'...</td>\n",
       "      <td>0.002886</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.724138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shiningthe</td>\n",
       "      <td>data/dialogs_preprocessed2/shiningthe_dialog.txt</td>\n",
       "      <td>['post', 'production', 'script', 'july', 'get'...</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.931034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>idesofmarchthe</td>\n",
       "      <td>data/dialogs_preprocessed2/idesofmarchthe_dial...</td>\n",
       "      <td>['write', 'george', 'clooney', 'grant', 'heslo...</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hangoverthe</td>\n",
       "      <td>data/dialogs_preprocessed2/hangoverthe_dialog.txt</td>\n",
       "      <td>['write', 'jon', 'lucas', 'scott', 'moore', 's...</td>\n",
       "      <td>0.004562</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bringingoutthedead</td>\n",
       "      <td>data/dialogs_preprocessed2/bringingoutthedead_...</td>\n",
       "      <td>['first', 'draft', 'paul', 'schrader', 'novel'...</td>\n",
       "      <td>0.002922</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                movie                                               path  \\\n",
       "0        donniebrasco  data/dialogs_preprocessed2/donniebrasco_dialog...   \n",
       "1          shiningthe   data/dialogs_preprocessed2/shiningthe_dialog.txt   \n",
       "2      idesofmarchthe  data/dialogs_preprocessed2/idesofmarchthe_dial...   \n",
       "3         hangoverthe  data/dialogs_preprocessed2/hangoverthe_dialog.txt   \n",
       "4  bringingoutthedead  data/dialogs_preprocessed2/bringingoutthedead_...   \n",
       "\n",
       "                                                text      hope  irritation  \\\n",
       "0  ['paul', 'attanasio', 'base', 'book', 'donnie'...  0.002886    0.000462   \n",
       "1  ['post', 'production', 'script', 'july', 'get'...  0.007365    0.000263   \n",
       "2  ['write', 'george', 'clooney', 'grant', 'heslo...  0.004899    0.000639   \n",
       "3  ['write', 'jon', 'lucas', 'scott', 'moore', 's...  0.004562    0.000480   \n",
       "4  ['first', 'draft', 'paul', 'schrader', 'novel'...  0.002922    0.000674   \n",
       "\n",
       "   ratio_hope_irritation  \n",
       "0               0.724138  \n",
       "1               0.931034  \n",
       "2               0.769231  \n",
       "3               0.809524  \n",
       "4               0.625000  "
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ratio_hope_irritation\"] = (df[\"hope\"] - df[\"irritation\"]) / (\n",
    "    df[\"hope\"] + df[\"irritation\"]\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab56bf6b-7eee-4eaa-806e-ffd5b722d4f3",
   "metadata": {},
   "source": [
    "### Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fea86f-119a-4ee8-9456-df6a7bc52c00",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">For this we would need the years of the movies</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "c052e3b4-b77f-4832-aa79-a2a1528bd9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
